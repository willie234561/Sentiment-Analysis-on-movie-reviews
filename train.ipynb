{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"i5doOG9iIsYg","colab_type":"text"},"source":["#**Sentiment Analysis on Movie Reviews 電影評論情緒分類**\n","\n","##    label分成五個類別,分別為下列資訊：\n","\n","*   0 → negative 負面\n","*   1 → Somewhat negative  一點點負面\n","*   2 → Neutral 一般\n","*   3 → Somewhat positive 一點點正面\n","*   4 → Positive 正面\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sXKj-j9ZfzFu","colab_type":"text"},"source":["### pytorch環境設定(https://pytorch.org)\n","選擇適合的版本並複製指令貼到本專案"]},{"cell_type":"code","metadata":{"id":"7mUPOCc9f1fT","colab_type":"code","outputId":"cf7ea41a-a5b6-467d-cf19-edd2ac65594a","executionInfo":{"status":"ok","timestamp":1587395971949,"user_tz":-480,"elapsed":5859,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["!pip install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6CQquGs0f2-E","colab_type":"code","outputId":"335eb54b-29bf-4c34-a252-a48a8fdd4787","executionInfo":{"status":"ok","timestamp":1587395975746,"user_tz":-480,"elapsed":9644,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["#確定有安裝成功\n","import torch\n","print(torch.__version__)\n","torch.cuda.is_available()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.4.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"KTf31QYifint","colab_type":"text"},"source":["### **取得 google drive 存取權限**"]},{"cell_type":"code","metadata":{"id":"8CqfyK8SIo2o","colab_type":"code","outputId":"73672c27-c945-44db-e838-25bb7a73b328","executionInfo":{"status":"ok","timestamp":1587396002609,"user_tz":-480,"elapsed":36496,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"72Pg2ymcGyg2","colab_type":"code","outputId":"40993e2d-ebc4-479f-f060-043a771707fe","executionInfo":{"status":"ok","timestamp":1587396004662,"user_tz":-480,"elapsed":38537,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive/Colab Notebooks/sentiment-analysis-on-movie-reviews/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/sentiment-analysis-on-movie-reviews\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KdtNtU4DG02O","colab_type":"code","outputId":"66ff36ce-6a06-4b92-b073-064379a454d1","executionInfo":{"status":"ok","timestamp":1587396008505,"user_tz":-480,"elapsed":42370,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["data   predict.ipynb  train_data.txt  train.ipynb\n","model  README.md      trained_model   train.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t_RZSbmA5iyI","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33rIRZCIj2yW","colab_type":"text"},"source":["### **下載dataset**"]},{"cell_type":"code","metadata":{"id":"_zDOYOkyj67s","colab_type":"code","colab":{}},"source":["import os\n","def fileExists(filepath):\n","  return  os.path.exists(filepath)\n","#從仲威的github中抓取資料集\n","if fileExists('train.tsv')!=True:\n","  !wget https://raw.githubusercontent.com/harry83528/sentiment-analysis-on-movie-reviews/master/data/train.tsv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ps1T6kk6j9s9","colab_type":"code","outputId":"d31d399d-8745-4ad5-8c4d-c15b153c8225","executionInfo":{"status":"ok","timestamp":1587396012821,"user_tz":-480,"elapsed":46620,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["data   predict.ipynb  train_data.txt  train.ipynb\n","model  README.md      trained_model   train.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OprYgB0GsrU3","colab_type":"code","outputId":"783d7b36-c511-4066-9607-4ec6f6f92864","executionInfo":{"status":"ok","timestamp":1587396013718,"user_tz":-480,"elapsed":47505,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["import os\n","import pandas as pd\n","df_train = pd.read_csv(\"train.tsv\", sep='\\t')\n","df_train.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PhraseId  ...  Sentiment\n","0         1  ...          1\n","1         2  ...          2\n","2         3  ...          2\n","3         4  ...          2\n","4         5  ...          2\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"OMXz16WmtTw0","colab_type":"code","outputId":"685b281e-ee14-4fce-d7a4-bbc764306e03","executionInfo":{"status":"ok","timestamp":1587396013719,"user_tz":-480,"elapsed":47491,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df_train.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(156060, 4)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"IYITsMe8tW8V","colab_type":"code","outputId":"b3070b0b-6936-4970-8b22-d4fc38fb8fc3","executionInfo":{"status":"ok","timestamp":1587396013719,"user_tz":-480,"elapsed":47478,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df_train.columns"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ru6ppx8dtepM","colab_type":"code","outputId":"8bc8c102-2b8f-43a1-d950-eb617c4e08d5","executionInfo":{"status":"ok","timestamp":1587396013720,"user_tz":-480,"elapsed":47467,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df_train.index"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RangeIndex(start=0, stop=156060, step=1)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"YeviSQW-tgp0","colab_type":"code","outputId":"d5e3886e-7087-40a7-a11a-54b1cef97b82","executionInfo":{"status":"ok","timestamp":1587396013720,"user_tz":-480,"elapsed":47455,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["df_train.info()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 156060 entries, 0 to 156059\n","Data columns (total 4 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   PhraseId    156060 non-null  int64 \n"," 1   SentenceId  156060 non-null  int64 \n"," 2   Phrase      156060 non-null  object\n"," 3   Sentiment   156060 non-null  int64 \n","dtypes: int64(3), object(1)\n","memory usage: 4.8+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fphap7s7thE4","colab_type":"code","outputId":"b98cb62a-e4f6-4c23-e226-38c0f2dbfca1","executionInfo":{"status":"ok","timestamp":1587396013721,"user_tz":-480,"elapsed":47442,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["df_train.describe()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>78030.500000</td>\n","      <td>4079.732744</td>\n","      <td>2.063578</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>45050.785842</td>\n","      <td>2502.764394</td>\n","      <td>0.893832</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>39015.750000</td>\n","      <td>1861.750000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>78030.500000</td>\n","      <td>4017.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>117045.250000</td>\n","      <td>6244.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>156060.000000</td>\n","      <td>8544.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            PhraseId     SentenceId      Sentiment\n","count  156060.000000  156060.000000  156060.000000\n","mean    78030.500000    4079.732744       2.063578\n","std     45050.785842    2502.764394       0.893832\n","min         1.000000       1.000000       0.000000\n","25%     39015.750000    1861.750000       2.000000\n","50%     78030.500000    4017.000000       2.000000\n","75%    117045.250000    6244.000000       3.000000\n","max    156060.000000    8544.000000       4.000000"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"KAR6i2HwgMr3","colab_type":"text"},"source":["### **將訓練資料tsv檔轉成txt檔(【\\t】分隔欄位)**\n","\n"]},{"cell_type":"code","metadata":{"id":"oc76UIGJgOAt","colab_type":"code","colab":{}},"source":["# 將要訓練的句子存起來\n","def SaveSentence(filepath, sent_list):\n","    f = open(filepath, 'w', encoding='UTF-8')\n","    for sent in sent_list:\n","        f.write(sent + '\\n')\n","    f.close()\n","\n","# 分割train_data和test_data\n","def data_Split(FileName):\n","\n","    fp = open(FileName, 'r', encoding='utf-8')\n","    line = fp.readline()        # 第一行是表頭PhraseId,SentenceId,Phrase,Sentiment\n","    line = fp.readline()        # 實際資料的第一行\n","\n","    test_data = []\n","\n","    # 用 while 逐行讀取檔案內容(一次讀一行)，直至檔案結尾\n","    while line:\n","        sent = ''\n","        sent = line.replace('\\n', '')\n","        test_data.append(sent)\n","        line = fp.readline()  #位移到下一行\n","    \n","    fp.close()\n","    \n","    SaveSentence('train_data.txt', test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hZkHhXEgP80","colab_type":"code","colab":{}},"source":["if fileExists('train_data.txt')!=True:\n","  data_Split('train.tsv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMHSR-23g14r","colab_type":"text"},"source":["### **安裝所需的函式庫-HuggingFace 團隊將 GitHub 專案大翻新並更名成 transformers(Install the Hugging Face Library)**\n","https://github.com/huggingface/transformers"]},{"cell_type":"code","metadata":{"id":"noekyOhAg3qt","colab_type":"code","outputId":"36b1feea-ec7c-483a-ae23-0abb0ad5913e","executionInfo":{"status":"ok","timestamp":1587478588426,"user_tz":-480,"elapsed":9934,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":655}},"source":["!pip install transformers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 8.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 16.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 25.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=f78df1e09ae0d5b7642f60304797cb34f2bc3919cc84e800ea0ac197562ffe51\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OKjsatBug5tU","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIfZwFodk2K_","colab_type":"text"},"source":["### **指定想要用的模型是哪一種**\n","到以下網址查詢模型與函數資訊\n","https://huggingface.co/transformers"]},{"cell_type":"code","metadata":{"id":"ADqD0SGTkOi1","colab_type":"code","colab":{}},"source":["PRETRAINED_MODEL_Folder =\"model/\"\n","PRETRAINED_MODEL_Name =\"albert-base-v1-pytorch_model.bin\"  # 指定 想要用的預訓練模型\n","PRETRAINED_Spiece_Name =\"albert-base-spiece.model\"\n","PRETRAINED_Config_Name =\"albert-base-config.json\"  #唯獨這個檔案自己下載好上傳並修改設定檔參數和增加num_labels=5\n","PRETRAINED_MODEL_Path =PRETRAINED_MODEL_Folder + PRETRAINED_MODEL_Name\n","PRETRAINED_Spiece_Path =PRETRAINED_MODEL_Folder + PRETRAINED_Spiece_Name\n","PRETRAINED_Config_Path =PRETRAINED_MODEL_Folder + PRETRAINED_Config_Name\n","trained_Model_Path = \"trained_model\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZscY-vcRlBXU","colab_type":"text"},"source":["### **下載想要的模型**"]},{"cell_type":"code","metadata":{"id":"61lbn5LnkEJ6","colab_type":"code","colab":{}},"source":["#創⼀個資料夾存bert預訓練模型\n","if not os.path.isdir(PRETRAINED_MODEL_Folder):\n","    os.mkdir(PRETRAINED_MODEL_Folder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kOwZ_OAkIZ-","colab_type":"code","colab":{}},"source":["import requests\n","\n","# 下載必要的模型檔案\n","# Download needed model files\n","\n","urls = ['https://s3.amazonaws.com/models.huggingface.co/bert/'+PRETRAINED_MODEL_Name,\n","        'https://s3.amazonaws.com/models.huggingface.co/bert/'+PRETRAINED_Spiece_Name]\n","filenames = [PRETRAINED_MODEL_Name,PRETRAINED_Spiece_Name]\n","\n","for i,url in enumerate(urls):\n","    r = requests.get(url)\n","    with open(PRETRAINED_MODEL_Folder+filenames[i], 'wb') as f:\n","        f.write(r.content)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRc990adg8W-","colab_type":"text"},"source":["## **引入需要⽤到的函式庫**"]},{"cell_type":"code","metadata":{"id":"FFqvPMRjg7nL","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import torch\n","#from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, AdamW\n","from transformers import AlbertConfig,AlbertTokenizer,AlbertForSequenceClassification,AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F # 激勵函數"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFROMAiKl8ui","colab_type":"text"},"source":["### **載入資料並將資料轉換成輸入格式(Token Embeddings、Segment Embeddings、Position Embeddings)**"]},{"cell_type":"code","metadata":{"id":"siiBtQ5nl-H0","colab_type":"code","colab":{}},"source":["def convert_data_to_feature(FileName):\n","    # 載入字典\n","    #tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","    #tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","    tokenizer = AlbertTokenizer.from_pretrained(PRETRAINED_Spiece_Path)\n","\n","    # 載入資料\n","    Labels = []\n","    Sentences = []\n","    SentencesId = []\n","    with open(FileName,'r',encoding='utf-8') as f:\n","        data = f.read()\n","    LS_pairs = data.split(\"\\n\")\n","    print('長度:',len(LS_pairs))\n","    #因為 label 與sentence在同⼀筆資料，所以我們將他們分開並個別存起來\n","    for LS_pair in LS_pairs:\n","        if LS_pair != \"\":\n","            try:\n","                #print(LS_pair)#2  1 A series of escapades demonstrating the adage that what is good for the goose 2\n","                LS_ary = LS_pair.split(\"\\t\")  #用tsv的【\\t】來分割欄位\n","                SId=LS_ary[0] #句子id        \n","                S = LS_ary[2] #句子\n","                L = LS_ary[3] #情緒label\n","                SentencesId.append(int(SId))  \n","                Sentences.append(S)\n","                Labels.append(int(L))\n","            except:\n","                continue\n","    \n","    assert len(Labels) == len(Sentences)  == len(SentencesId)\n","    print('Sentences:',Sentences[0:20])\n","    # BERT input embedding\n","    max_seq_len = 0     # 紀錄最大長度\n","    input_ids = []\n","    original_length = []    # 紀錄原本長度\n","    #==轉換成BERT input 需要的格式===============================================\n","    for S in Sentences:\n","        # 將句子切割成一個個token\n","        word_piece_list = tokenizer.tokenize(S)\n","        # 將token轉成字典中的id\n","        input_id = tokenizer.convert_tokens_to_ids(word_piece_list)\n","        # 補上[CLS]和[SEP]\n","        input_id = tokenizer.build_inputs_with_special_tokens(input_id)\n","\n","        if(len(input_id)>max_seq_len):\n","            max_seq_len = len(input_id)\n","        input_ids.append(input_id)\n","        #反向查詢(還原)\n","        #print(tokenizer.convert_ids_to_tokens(tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_id)))))\n","    #==轉換成BERT input 需要的格式  END==========================================\n","    \n","    print(\"最長句子長度:\",max_seq_len)\n","    \n","    assert max_seq_len <= 512 # 小於BERT-base長度限制\n","    max_seq_len = 512\n","\n","    #統⼀輸入資料的長度==========================================================\n","    # 補齊長度\n","    for c in input_ids:\n","        # 紀錄原本長度\n","        length = len(c)\n","        original_length.append(length)\n","        while len(c)<max_seq_len:\n","            c.append(0)\n","    #⽣成每⼀句的type_id\n","    token_type_ids = [[0]*max_seq_len for i in range(len(Sentences))]        # token_type_ids # 儲存的是句子的id，id為0就是第一句，id為1就是第二句\n","    attention_mask = []                                                      # attention_mask # 1代表是真實的單詞id，0代表補齊長度\n","    for i in range(len(Sentences)):\n","        attention_id = []\n","        for j in range(original_length[i]):\n","            attention_id.append(1)\n","        while len(attention_id)<max_seq_len:\n","            attention_id.append(0)\n","        attention_mask.append(attention_id)\n","\n","    assert len(input_ids) == len(token_type_ids) and len(input_ids) == len(attention_mask) and len(input_ids) == len(Labels) and len(input_ids) == len(SentencesId)\n","\n","    #為了使⽤⽅便，把三個input embedding及 label 包成 object======================\n","    data_features = {'input_ids':input_ids,\n","                    'token_type_ids':token_type_ids,\n","                    'attention_mask':attention_mask,\n","                    'labels':Labels,\n","                    'sentences_ids':SentencesId}\n","\n","    print('sentences_ids:',SentencesId[0:10])\n","    print('input_ids:',input_ids[0:10])\n","    print('Labels:',Labels[0:50])\n","    return data_features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJa9hDtHha7d","colab_type":"text"},"source":["### **輸入格式轉Dataset**"]},{"cell_type":"code","metadata":{"id":"MxpS9LuRhcit","colab_type":"code","colab":{}},"source":["def makeDataset(data_feature):\n","    input_ids = data_feature['input_ids']\n","    token_type_ids = data_feature['token_type_ids']\n","    attention_mask = data_feature['attention_mask']\n","    labels = data_feature['labels']\n","    sentences_ids = data_feature['sentences_ids']\n","\n","    all_input_ids = torch.tensor([input_id for input_id in input_ids], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([token_type_id for token_type_id in token_type_ids], dtype=torch.long)\n","    all_attention_mask_ids = torch.tensor([attention_id for attention_id in attention_mask], dtype=torch.long)\n","    all_labels = torch.tensor([label for label in labels], dtype=torch.long)\n","    all_sentences_ids = torch.tensor([sentences_id for sentences_id in sentences_ids], dtype=torch.long)\n","    dataset = TensorDataset(all_input_ids, all_token_type_ids, all_attention_mask_ids, all_labels ,all_sentences_ids)\n","\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GD-OABDMhe8E","colab_type":"text"},"source":["## **確定 Embedding 是否正確**"]},{"cell_type":"code","metadata":{"id":"eALXndj9hgDt","colab_type":"code","outputId":"25cf0893-4ee7-43f9-cd46-2f4667100ab3","executionInfo":{"status":"error","timestamp":1587478548542,"user_tz":-480,"elapsed":1067,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["#tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","# 取得此預訓練模型所使用的 tokenizer\n","#tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","tokenizer = AlbertTokenizer.from_pretrained(PRETRAINED_Spiece_Path)\n","data_features = convert_data_to_feature('train_data.txt')\n","\n","print(data_features['input_ids'][43636])\n","print(tokenizer.convert_ids_to_tokens(data_features['input_ids'][43636]))\n","print(data_features['token_type_ids'][43636])\n","print(data_features['attention_mask'][43636])\n","print(data_features['labels'][43636])\n","print(data_features['sentences_ids'][43636])"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-10710e80cd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRETRAINED_Spiece_Path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data_to_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m43636\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m43636\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AlbertTokenizer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"bJvLZ1tH2rgf","colab_type":"text"},"source":["## **分割數據**\n","把資料切成訓練集跟測試集\n","對於 Supervised Learning 來說，我們使⽤標註資料（Labeled Data）來做訓練\n","因此我們需要訓練資料（Training Data）\n","但是我們蒐集到的資料卻不能全部拿來做訓練\n","因為我們必須要保留⼀些當作測試資料（Testing Data）來評估模型表現\n","這些資料必須跟訓練資料是完全不同的，否則就有作弊的嫌疑\n","所以我們會把我們的資料切成訓練集與測試集，通常會保留比較多當作訓練資料\n","（60%~80%），其餘做測試資料"]},{"cell_type":"code","metadata":{"id":"ST5ZHdBt2BA9","colab_type":"code","colab":{}},"source":["def split_dataset(full_dataset, split_rate=0.8):  \n","    train_size = int(split_rate * len(full_dataset))\n","    test_size = len(full_dataset) - train_size\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","    return train_dataset,test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXGgr6iUhiRd","colab_type":"text"},"source":["### **Fine-Tuning**\n"]},{"cell_type":"code","metadata":{"id":"dN6qIwfDhlVV","colab_type":"code","colab":{}},"source":["# 計算正確值\n","def compute_accuracy(y_pred, y_target):\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0d-v6Hchni2","colab_type":"code","outputId":"3932eb63-ea97-4fa7-89a9-0778950181d0","executionInfo":{"status":"ok","timestamp":1587396116659,"user_tz":-480,"elapsed":150323,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":171}},"source":["#設定模型參數 Start =============================================================\n","\n","#bert_config, bert_class, bert_tokenizer = (BertConfig, BertForSequenceClassification, BertTokenizer)\n","\n","\n","#Step1:初始化加載模型 Start======================================================\n","config = AlbertConfig.from_json_file(PRETRAINED_Config_Path)  #這步驟非常正要,要指定label數,不然預設是二分類\n","model = AlbertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_Path, config = config)\n","#config = bert_config.from_pretrained('bert-base-cased',num_labels = 5)  #這步驟非常正要,要指定label數,不然預設不會是多分類\n","#model = bert_class.from_pretrained('bert-base-cased', from_tf=bool('.ckpt' in 'bert-base-cased'), config=config)  #這步驟非常正要,要指定label數,不然預設會是2分類\n","#model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n","#Step1:初始化加載模型 END========================================================\n","\n","#Step2:指定硬體裝置 Start========================================================\n","# setting device\n","#你電腦的 GPU 能否被 PyTorch 調用,如果不行就使用CPU \n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"using device\",device)\n","model.to(device)\n","#Step2:指定硬體裝置 END==========================================================\n","\n","#Step3:將訓練資料讀入並且組建BERT輸入格式 Start===================================\n","train_data_feature = convert_data_to_feature('train_data.txt')\n","#for i in range(500):\n","#print(data_features['labels'][i],sep=\",\")\n","#test_data_feature = convert_data_to_feature('test_data.txt')\n","full_dataset = makeDataset(train_data_feature)\n","#test_dataset = makeDataset(test_data_feature)\n"," #Step3:將訓練資料讀入並且組建BERT輸入格式 END====================================\n","\n","#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader Srart======\n","train_dataset, test_dataset = split_dataset(full_dataset, 0.8)\n","train_dataloader = DataLoader(train_dataset ,batch_size=4 ,shuffle=True)\n","test_dataloader = DataLoader(test_dataset ,batch_size=4 ,shuffle=True)\n","#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader END========\n","\n","#準備優化器 Start===============================================================\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","Learning_rate = 5e-6       # 學習率\n","optimizer = AdamW(optimizer_grouped_parameters, lr=Learning_rate, eps=1e-8)\n","#準備優化器 End=================================================================\n","\n","\n","#設定模型參數 End==============================================================="],"execution_count":28,"outputs":[{"output_type":"stream","text":["using device cuda\n"],"name":"stdout"},{"output_type":"stream","text":["Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"],"name":"stderr"},{"output_type":"stream","text":["長度: 156061\n","Sentences: ['A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .', 'A series of escapades demonstrating the adage that what is good for the goose', 'A series', 'A', 'series', 'of escapades demonstrating the adage that what is good for the goose', 'of', 'escapades demonstrating the adage that what is good for the goose', 'escapades', 'demonstrating the adage that what is good for the goose', 'demonstrating the adage', 'demonstrating', 'the adage', 'the', 'adage', 'that what is good for the goose', 'that', 'what is good for the goose', 'what', 'is good for the goose']\n","最長句子長度: 93\n","sentences_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","input_ids: [[2, 21, 231, 16, 13, 13944, 8240, 160, 16133, 14, 21, 43, 1303, 30, 98, 25, 254, 26, 14, 13669, 25, 67, 254, 26, 14, 19383, 106, 13, 15, 109, 16, 56, 4533, 21, 25443, 18, 47, 2369, 16, 56, 8545, 20, 212, 16, 21, 609, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 21, 231, 16, 13, 13944, 8240, 160, 16133, 14, 21, 43, 1303, 30, 98, 25, 254, 26, 14, 13669, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 21, 231, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 21, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 231, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 16, 13, 13944, 8240, 160, 16133, 14, 21, 43, 1303, 30, 98, 25, 254, 26, 14, 13669, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 16, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 13, 13944, 8240, 160, 16133, 14, 21, 43, 1303, 30, 98, 25, 254, 26, 14, 13669, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 13, 13944, 8240, 160, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 16133, 14, 21, 43, 1303, 30, 98, 25, 254, 26, 14, 13669, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","Labels: [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xJE0E8pEhs0E","colab_type":"text"},"source":["### **開始訓練**"]},{"cell_type":"code","metadata":{"id":"LejuOe_qhyFG","colab_type":"code","outputId":"00b56457-d973-4248-8f9c-c0d611125916","executionInfo":{"status":"ok","timestamp":1587403522306,"user_tz":-480,"elapsed":5551339,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Step5:建構training loop、開始訓練 Start=========================================\n","for epoch in range(1):\n","    \n","    All_train_correct = 0.0\n","    AllTrainLoss = 0.0\n","    count = 0\n","    model.zero_grad() #梯度歸零\n","    for batch_dict in train_dataloader:\n","\n","        # 訓練模式\n","        model.train()\n","        \n","        batch_dict = tuple(t.to(device) for t in batch_dict)\n","        #print(batch_dict[0].shape)\n","        #print(batch_dict[1].shape)\n","        #print(batch_dict[2].shape)\n","        #print(batch_dict[3].shape)\n","        #print(batch_dict[4].shape)\n","        #順序對應產⽣Dataset那裡的dataset = TensorDataset(all_input_ids, all_segment_ids,all_position_ids, all_labels ,all_sentences_ids)\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],             \n","            attention_mask = batch_dict[2],             \n","            labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]\n","            \n","        train_correct = compute_accuracy(logits, batch_dict[3])       # 計算正確率\n","        All_train_correct += train_correct\n","        AllTrainLoss += loss.item()\n","        count += 1\n","\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","            \n","    Average_train_correct = round(All_train_correct/count, 3)\n","    Average_train_loss = round(AllTrainLoss/count, 3)\n","\n","    # 測試模式\n","    model.eval()\n","    All_test_correct = 0.0\n","    AllTestLoss = 0.0\n","    count = 0\n","    for batch_dict in test_dataloader:\n","        batch_dict = tuple(t.to(device) for t in batch_dict)  #每⼀個Batch把資料從CPU RAM 推到 GPU RAM\n","\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],            \n","            attention_mask = batch_dict[2],           \n","            labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]\n","\n","        test_correct = compute_accuracy(logits, batch_dict[3])       # 計算正確率\n","        All_test_correct += test_correct\n","        AllTestLoss += loss.item()\n","\n","        count += 1\n","        \n","    Average_test_correct = round(All_test_correct/count, 3)\n","    Average_test_loss = round(AllTestLoss/count, 3)\n","\n","    print('第' + str(epoch+1) + '次' + '訓練模式，loss為:' + str(Average_train_loss) + ' 正確率為' + str(Average_train_correct)+ '，測試模式，loss為:' + str(Average_test_loss) + ' 正確率為' + str(Average_test_correct))\n","\n","#Step5:建構training loop、開始訓練 END==========================================="],"execution_count":29,"outputs":[{"output_type":"stream","text":["第1次訓練模式，loss為:0.884 正確率為63.341，測試模式，loss為:0.811 正確率為65.776\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"steBZuo9h28-","colab_type":"text"},"source":["### **儲存模型**"]},{"cell_type":"code","metadata":{"id":"iUB2Wpaxh3Sm","colab_type":"code","cellView":"both","colab":{}},"source":["#創⼀個資料夾存模型\n","if not os.path.isdir(trained_Model_Path):\n","    os.mkdir(trained_Model_Path)\n","#儲存\n","model_to_save = model.module if hasattr(model, 'module') else model\n","model_to_save.save_pretrained(trained_Model_Path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSBKAM1lRE_D","colab_type":"code","colab":{}},"source":["#predict 時把輸入的⽂字轉成輸入格式\n","def to_input_id(sentence_input):\n","    return tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_input)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GR-ik7W1RJDc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"894f0b7d-fcba-4789-d045-7cab41aeb397","executionInfo":{"status":"error","timestamp":1587478617187,"user_tz":-480,"elapsed":4276,"user":{"displayName":"顧哲瑋","photoUrl":"","userId":"10784526409997437695"}}},"source":["# load and init\n","#將輸入的句⼦轉為wordpices格式並載入我們train好的模型\n","#tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","# 取得此預訓練模型所使用的 tokenizer\n","tokenizer = AlbertTokenizer.from_pretrained(PRETRAINED_Spiece_Path)\n","\n","config = AlbertConfig.from_json_file(trained_Model_Path+'/config.json') #預測時用訓練產出的設定檔\n","#config = AlbertConfig.from_pretrained(PRETRAINED_MODEL_Path,num_labels = 5)  #這步驟非常正要,要指定label數,不然預設是二分類\n","model = AlbertForSequenceClassification.from_pretrained(trained_Model_Path+'/pytorch_model.bin', config = config) #預測時用訓練產出的模型\n","\n","\n","model.eval()\n","\n","print('請輸入句子')\n","sentence = 'nice'\n","\n","input_id = to_input_id(sentence)\n","assert len(input_id) <= 512\n","\n","while len(input_id)<512:\n","    input_id.append(0)\n","\n","input_ids = torch.LongTensor(input_id).unsqueeze(0)\n","\n","# predict時，因為沒有label所以沒有loss\n","outputs = model(input_ids)\n","print('1',outputs,type(outputs))\n","predicts = outputs[:2]\n","print('2',predicts,type(predicts))\n","predicts = predicts[0]\n","print('3',predicts,type(predicts))\n","max_val = torch.max(predicts)\n","print('4',max_val,type(max_val))\n","predict_label = (predicts == max_val).nonzero().numpy()[0][1] # 在第1維度取最大值並返回索引值\n","\n","print('label:'+str(predict_label))\n","\n","if str(predict_label) == '0':\n","  print('Negative')\n","elif str(predict_label) == '1':\n","  print('Somewhat negative')\n","elif str(predict_label) == '2':\n","  print('Neutral')\n","elif str(predict_label) == '3':\n","  print('Somewhat positive')\n","elif str(predict_label) == '4':\n","  print('Positive')\n","else:\n","  print('程式發生錯誤')"],"execution_count":8,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-572d01f7b709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRETRAINED_Spiece_Path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_Model_Path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/config.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#預測時用訓練產出的設定檔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#config = AlbertConfig.from_pretrained(PRETRAINED_MODEL_Path,num_labels = 5)  #這步驟非常正要,要指定label數,不然預設是二分類\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_Model_Path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/pytorch_model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#預測時用訓練產出的模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 )\n\u001b[1;32m    498\u001b[0m             )\n","\u001b[0;31mOSError\u001b[0m: Model name 'model/albert-base-spiece.model' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'model/albert-base-spiece.model' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url."]}]},{"cell_type":"code","metadata":{"id":"chB6POy4sMsu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}